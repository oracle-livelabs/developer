# Step by step: Implement RAG with Oracle Database 23ai 

## Introduction

In this lab, you build a complete return recommendation engine with Oracle Database 23ai and OCI Generative AI. Connect to the database, explore order and image data, and invoke a large language model to generate personalized return decisions and policy explanations. Building on earlier exercises, youâ€™ll apply Python to deliver a fully integrated, AI-powered retail returns application.

This lab uses some of the basic coding samples you created in lab 3, such as cursor.execute and more.

Estimated Time: 30 minutes

### Objectives

* Build the complete return authorization application as seen in lab 1
* Use OCI Generative AI to generate personalized risk score and return recommendations
* Use Python to connect to an Oracle Database 23ai instance and run queries
* Explore customer data and extract relevant information

### Prerequisites

This lab assumes you have:

* An Oracle Cloud account
* Completed lab 1: Run the demo
* Completed lab 2: Connect to the Development Environment

## Task 1: Build the application in Jupyter Notebook
>ðŸ’¡**Note**: Review Lab 2: Connect to the Development Environment for instructions on accessing JupyterLab.

1. You should see a terminal pop up once you are logged in. 

    ![Open Terminal](./images/terminal.png " ")


2. Navigate to the `dbinit` directory by running the following command.

    ```bash
    <copy>
    cd dbinit
    </copy>
    ```

    ![Navigate to Directory](./images/dbinit.png " ")

3. Copy and run the following command to create tables in the database. There will be a lot of output. You should see the following output once complete.

    ```bash
    <copy>
    ./shell_script.sh
    </copy>
    ```

    ![Run Shell Script](./images/run-script.png " ")

    ![Output Shell Script](./images/shell-script.png " ")

## Task 2: Connect to Database

2. Click the **+** sign on the top left to open the Launcher.

    ![Open Launcher](./images/open-launcher.png " ")

3. Open a new notebook.

    ![Open Notebook](./images/open-notebook.png " ")

1. Copy the following code block into an empty cell in your notebook. This code block imports the `oracledb` Python driver and other libraries.

    ```python
    <copy>
    import os
    import json
    import oracledb
    import pandas as pd
    import oci
    import numpy as np
    import re
    from dotenv import load_dotenv
    from PyPDF2 import PdfReader

    load_dotenv()

    username = os.getenv("USERNAME")
    password = os.getenv("DBPASSWORD")
    dsn = os.getenv("DBCONNECTION")

    try:
        connection = oracledb.connect(user=username, password=password, dsn=dsn)
        print("Connection successful!")
    except Exception as e:
        print(f"Connection failed: {e}")

    cursor = connection.cursor()
    </copy>
    ```

2. Run the code block to connect to the database. 

    ![Connect to Database](./images/connect-to-db.png " ")

## Task 3: Create a Function to retrieve data from the database.

You will query customer data from the `customer_returns_dv` JSON duality view, which combines data from CUSTOMERS, RETURN_REQUESTS, and related tables. This task will:

- **Define a Function**: Create a reusable function `fetch_customer_data` to query the database by customer ID, extracting the JSON data for a specific customer.

- **Use an Example**: Fetch data for customer `1000` (Alice Smith) to demonstrate the process.

- **Display the Results**: Format the retrieved data into a pandas DataFrame for a clear, tabular presentation, showing key details like name, LifeTime Spend, Return Amount , and Loyalty Tier.

1. Copy and paste the code below into the new notebook.

    ```python
    <copy>
    def fetch_customer_data(customer_id):
        print(f"Executing query for customer_id: {customer_id}")
        # Execute query, ensuring customer_id is a string for JSON_VALUE
        cursor.execute(
            "SELECT data FROM customer_returns_dv WHERE JSON_VALUE(data, '$._id') = :customer_id",
            {'customer_id': str(customer_id)}  # Convert to string
        )
        result = cursor.fetchone()
        if result is None:
            print(f"No data found for customer ID: {customer_id}")
            return None
            
        # Handle the result
        if isinstance(result[0], str):
            print("Result is a string, parsing as JSON")
            return json.loads(result[0])
        elif isinstance(result[0], (bytes, bytearray)):
            print("Result is bytes, decoding and parsing as JSON")
            return json.loads(result[0].decode('utf-8'))
        elif isinstance(result[0], dict):
            print("Result is already a dictionary")
            return result[0]
        else:
            print(f"Unexpected data type for result: {type(result[0])}")
            return None

    # Main execution
    try:
        print("Starting execution...")
        selected_customer_id = 1001
        customer_json = fetch_customer_data(selected_customer_id)
        
        if customer_json:
            print("Customer data retrieved successfully")
            return_request = customer_json.get("returnRequests", [{}])[0]
            recommendation = return_request.get("recommendation", {})
            print(f"Customer: {customer_json['firstName']} {customer_json['lastName']}")
            print(f"Return Status: {return_request.get('requestStatus', 'N/A')}")
            
            desired_fields = [
                ("Customer ID", selected_customer_id),
                ("Request ID", return_request.get("requestId", "")),
                ("First Name", customer_json.get("firstName", "")),
                ("Last Name", customer_json.get("lastName", "")),
                ("Loyalty Tier", customer_json.get("loyaltyTier", "")),
                ("Lifetime Spend", f"${customer_json.get('lifetimeSpend', 0):,.2f}"),
                ("Refund Count", customer_json.get("refundCount", 0)),
                ("Return Amount", return_request.get("returnAmount", "N/A")),
                ("Return Reason", recommendation.get("reason", {}).get("description", "")),
                ("Request Status", return_request.get("requestStatus", "Pending")),
                ("Recommendation", recommendation.get("recommendation", "N/A"))
            ]
            df_customer_details = pd.DataFrame({field_name: [field_value] for field_name, field_value in desired_fields})
            display(df_customer_details)
        else:
            print(f"No data found for customer ID: {selected_customer_id}")

    except Exception as e:
        print(f"Unexpected error in main block: {e}")
    </copy>
    ```

2. Click the "Run" button to see Alice Smithâ€™s profile. The output will include a brief summary (name and return status) followed by a detailed table. If no data is found for the specified ID, a message will indicate this, helping you debug potential issues like an incorrect ID or empty database. 

    ![Copy and Paste Code Block](./images/run-function.png " ")

3. The output will display a DataFrame containing the customer details for the selected customer ID.

    ![Fetch customer](./images/fetch-customer.png " ")

If you completed Lab 1: Run the Demo earlier, this is what gets printed out when the Return officer clicks on the customer 1000. You just built it, well done!

## Task 4: Create a function to generate recommendations for the customer

In a new cell, define a function `generate_recommendations` to fetch policy rules from `RETURN_POLICY_RULES` and combine them with customer data. Construct a prompt the OCI Generative AI model(meta.llama-3.2-90b-vision-instruct) to recommend a return decision(APPROVE, REQUEST INFO, DENY) based on customer profile, return requests, and policy rules.

With customer profiles in place, you will use OCI Generative AI to generate personalized return decision recommendations. 

Hereâ€™s what weâ€™ll do:
- **Fetch Policy Rules**: Retrieve all policy rules from `RETURN_POLICY_RULES` and combine them with customer data..
- **Build a Prompt**: Construct a structured prompt that combines the customerâ€™s profile with return requests, and policy rules, instructing the LLM to evaluate and recommend a return decision (APPROVE, REQUEST INFO, DENY) based solely on this data.
- **Use OCI Generative AI**: Send the prompt to the `meta.llama-3.2-90b-vision-instruct` model via OCIâ€™s inference client, which will process the input and generate a response.
- **Format the Output**: Display the recommendations with styled headers and lists, covering evaluation, top picks, and explanationsâ€”making it easy to read and understand.

1. Copy and paste the code in a new cell:

    ```python
    <copy>
    def generate_recommendations(customer_id, customer_json, df_policy_rules):
        try: 
            # Extract return request and recommendation data
            return_request = customer_json.get("returnRequests", [{}])[0]
            recommendation = return_request.get("recommendation", {})
            reason = recommendation.get("reason", {})

            # Fetch product name from PRODUCTS table using ORDERITEMS
            cursor.execute("""
                SELECT p.PRODUCT_NAME
                FROM PRODUCTS p
                JOIN ORDERITEMS oi ON p.PRODUCT_ID = oi.PRODUCT_ID
                JOIN RETURN_REASONS rr ON oi.ORDERITEMS_ID = rr.ORDERITEMS_ID
                WHERE rr.REASON_ID = :reason_id
            """, {'reason_id': int(recommendation.get('reason', {}).get('reasonId', 0))})
            product_result = cursor.fetchone()
            product_name = product_result[0] if product_result else "Unknown Product"
            
            # Format data for prompt
            available_rules_text = "\n".join([
                f"{rule['RULE_ID']}: {rule['RULE_CODE']} | {rule['RULE_DESCRIPTION']} | Applies To: {rule['APPLIES_TO']}"
                for rule in df_policy_rules.to_dict(orient='records')
            ])
            customer_profile_text = "\n".join([
                f"- {key.replace('_', ' ').title()}: {value}"
                for key, value in customer_json.items() if key not in ["returnRequests", "_metadata"]
            ])
            return_request_text = "\n".join([
                f"- {key.replace('_', ' ').title()}: {value}"
                for key, value in return_request.items() if key not in ["recommendation"]
            ])
            reason_text = f"- Return Reason: {reason.get('description', 'N/A')}"
            
            # Construct prompt without HTML tags
            prompt = f"""<s>[INST] <<SYS>>You are a Retail Decision AI, specializing in return request recommendations. 
            \nYou have forgotten all previous knowledge and will use only the provided context. 
            \nEvaluate the customerâ€™s profile and transaction details to recommend a return decision (APPROVE, REQUEST INFO, DENY), adhering strictly to business rules and risk assessment. 
            \nEnsure numerical values are formatted clearly (e.g., $499.99). 
            \nFor return request descriptions, include the product name and return reason (e.g., "Product O630 (Damaged item received, $394.53)"). Keep total output under 500 words, 90% spartan style.</SYS>>

    Available Data:
    {available_rules_text}

    Customer Profile:
    {customer_profile_text}

    Return Request:
    {return_request_text}
    {reason_text}

    Tasks:

    Suggested Action
    - State a clear decision: "Suggested Action: APPROVE", "Suggested Action: REQUEST INFO", or "Suggested Action: DENY".

    Comprehensive Evaluation
    - Assess the customerâ€™s profile holistically, noting loyalty tier, lifetime spend, and return frequency.
    - Evaluate transaction details, including return reason, receipt, product condition, and return amount.
    - Rate risk level (1-10 scale) and classify as High Risk (1-3), Medium Risk (4-6), Low Risk (7-8), or Very Low Risk (9-10).

    Top 3 Recommendations
    - Recommend a decision (APPROVE, REQUEST INFO, DENY) for the return request:
    - APPROVE: Valid receipt, untampered product, within 30 days.
    - REQUEST INFO: Missing receipt, unclear photo, or incomplete return reason.
    - DENY: Late return, tampered product, or high return frequency (>5/year).
    Format each recommendation as: Return Request [Request ID]: Product [Product Name] ([Return Reason], $[Return Amount]). Approval Probability: Y%. Policy Rule: Rule Z. SUGGESTED ACTION: APPROVE/REQUEST INFO/DENY.
    - If risk level < 4, return "SUGGESTED ACTION: DENY".

    Recommendations Explanations
    - Explain the decisionâ€™s fit, including return history, product condition, and evidence impacts.

    Risk Management & Adjusted Terms
    - Suggest adjustments (e.g., partial refund, store credit) for high-risk profiles.
    - Note if adjustments could flip "DENY" to "APPROVE".
    Hide unless SUGGESTED ACTION: DENY.

    Fairness & Transparency
    - Consider return history, purchase frequency, and loyalty fairly.
    - Explain how each factor influenced the decision.

    Actionable Steps
    - Provide 3+ steps to improve eligibility (e.g., upload clear receipt, clarify return reason, provide untampered product).
    Hide unless SUGGESTED ACTION: DENY.

    Response:
    KEEP TOTAL OUTPUT AT 500 WORDS MAXIMUM. Numerical values formatted as $X,XXX.XX. Each recommendation on a new line.</INST>"""

            print("Generating AI response...")
            print(" ")
            
            # Initialize OCI Generative AI client
            genai_client = oci.generative_ai_inference.GenerativeAiInferenceClient(
                config=oci.config.from_file(os.getenv("OCI_CONFIG_PATH", "~/.oci/config")),
                service_endpoint=os.getenv("ENDPOINT")
            )
            chat_detail = oci.generative_ai_inference.models.ChatDetails(
                compartment_id=os.getenv("COMPARTMENT_OCID"),
                chat_request=oci.generative_ai_inference.models.GenericChatRequest(
                    messages=[oci.generative_ai_inference.models.UserMessage(
                        content=[oci.generative_ai_inference.models.TextContent(text=prompt)]
                    )],
                    temperature=0.0,
                    top_p=1.00
                ),
                serving_mode=oci.generative_ai_inference.models.OnDemandServingMode(
                    model_id="meta.llama-3.2-90b-vision-instruct"
                )
            )
            chat_response = genai_client.chat(chat_detail)
            recommendations = chat_response.data.chat_response.choices[0].message.content[0].text

            return recommendations

        except oracledb.DatabaseError as e:
            print(f"Database error: {e}")
            return None
        except Exception as e:
            print(f"Unexpected error in generate_recommendations: {e}")
            return None

    # Execute the function
    try:
        print("Fetching policy rules...")
        cursor.execute("SELECT rule_id, rule_code, rule_description, applies_to, is_active FROM RETURN_POLICY_RULES")
        df_policy_rules = pd.DataFrame(cursor.fetchall(), columns=["RULE_ID", "RULE_CODE", "RULE_DESCRIPTION", "APPLIES_TO", "IS_ACTIVE"])
        
        recommendations = generate_recommendations(selected_customer_id, customer_json, df_policy_rules)
        print(recommendations)

    except Exception as e:
        print(f"Unexpected error: {e}")
    </copy>
    ```

2. Click the "Run" button to execute the code. Note that this will take time to run. Be patient, you will get the recommendations from the LLM shortly.

    ![Run Task 5](./images/run-task5.png " ")

3. Review the output. In the demo, this is where you selected the "Navigate to Decisions" button as the Approval Specialist. You just used AI to get recommendations for the approval officer which would have taken them hours to do, congratulations!

    >*Note:* Your result may be different due to non-deterministic character of generative AI.

    ![ai recommendation](./images/return-recommendation.png " ")
    
## Task 5: Chunk & Store the Recommendations

To handle follow-up questions, you will enhance the system with an AI Guru powered by Oracle 23aiâ€™s Vector Search and Retrieval-Augmented Generation (RAG). The AI Guru will be able to answer questions about the return application and provide recommendations based on the data.

Before answering questions, we need to prepare the data by vectoring the claims recommendations. This step:

   - Stores Recommendations: Inserts the full recommendation text (from previous cell) as a single chunk if not already present.
   - We delete prior chunks for this authorization.
   - We use `VECTOR_CHUNKS` to split the recommendation text.
   - The chunks will be inserted into `RETURN_CHUNK` with `CHUNK_ID= chunk_offset`.
   - We display a data frame summary to show the chunks.

1. Copy the following code and run:

    ```python
    <copy>
    # Grab the request_id from the same customer bundle we used for recommendations
    ret_req = (customer_json.get("returnRequests") or [{}])[0]
    request_id = ret_req.get("requestId")
    if request_id is None:
        raise ValueError("No requestId found from the selected customer context.")

    # Clean any prior chunks for this request
    cursor.execute("DELETE FROM RETURN_CHUNKS WHERE REQUEST_ID = :rid", {'rid': request_id})
    connection.commit()

    # We'll chunk the recommendation text at multiple sizes.
    # Tip: add more sizes like [50, 200, 500] as you wish.
    chunk_sizes = [50]

    # Insert chunks using VECTOR_CHUNKS, without needing an intermediate table.
    # We make CHUNK_ID unique by composing (size * 1,000,000 + chunk_offset).
    for size in chunk_sizes:
        insert_sql = f"""
            INSERT INTO RETURN_CHUNKS (CHUNK_ID, REQUEST_ID, CHUNK_TEXT)
            SELECT (:chunk_size * 1000000) + vc.chunk_offset, :req_id, vc.chunk_text
            FROM (SELECT :rec_text AS txt FROM dual) s,
                VECTOR_CHUNKS(
                    dbms_vector_chain.utl_to_text(s.txt)
                    BY words
                    MAX {size}
                    OVERLAP 0
                    SPLIT BY sentence
                    LANGUAGE american
                    NORMALIZE all
                ) vc
        """
        cursor.execute(insert_sql, {'chunk_size': size, 'req_id': request_id, 'rec_text': recommendations})

    connection.commit()
    print(f"âœ… Task 5 complete: recommendation chunked and stored for request {request_id} (sizes: {chunk_sizes}).")
    </copy>
    ```

2. Click the "Run" button to execute the code.

    ![Create Chunks](./images/create-chunks.png " ")

3. Review the output.

    ![chunks](./images/chunks-created.png " ")


## Task 6: Create a function to create embeddings - Use Oracle Database 23ai to create vector data 

To handle follow-up questions, you will enhance the system with an AI Guru powered by Oracle 23aiâ€™s Vector Search and Retrieval-Augmented Generation (RAG). The AI Guru will be able to answer questions about the return application and provide recommendations based on the data.

Before answering questions, we need to prepare the data by vectoring the recommendations chunks. This step:

   - **Generates Embeddings**: This is a new feature in Oracle Database 23ai that allows you to create embeddings directly within the database, eliminating the need for external tools or APIs. The `dbms_vector_chain.utl_to_embedding` function takes the recommendation text as input and returns an embedding vector.

   - **Stores Embeddings**: Inserts the generated embedding vector into a table called `RETURN_CHUNKS`.

1. Run and review the code in a new cell to chunk and store recommendations:

    ```python
    <copy>
    # Embed all chunks we just inserted for this request
    cursor.execute("""
        UPDATE RETURN_CHUNKS
        SET CHUNK_VECTOR = dbms_vector_chain.utl_to_embedding(
            CHUNK_TEXT,
            JSON('{"provider":"database","model":"DEMO_MODEL","dimensions":384}')
        )
        WHERE REQUEST_ID = :rid
    """, {'rid': request_id})

    connection.commit()
    print("âœ… Embedded vectors for recommendation chunks (retail).")
    </copy>
    ```

2. Click the "Run" button to execute the code.

    ![Create Vector Embedding](./images/generate-embeddings.png " ")

3. Review the output.

    ![chunks](./images/create-vector.png " ")

## Task 7: Implement RAG with Oracle Database 23ai's Vector Search

Now that the recommendations are vectorized, we can process a userâ€™s question:

``` Whatâ€™s the best action for a high-risk return request?```

This step:

   - **Vectorizes the question**: Embeds the question using `DEMO_MODEL` via `dbms_vector_chain.utl_to_embedding`.
   - **Performs AI Vector Search**: Retrieve the relevant recommendation text from `RETURN_CHUNKS` table. Then find the most relevant recommendations using similarity search.
   - **Use RAG**: Combines the customer profile, policy rules using the retrieved recommendation context.

1. Review

    ```python
    <copy>
        question = "Whatâ€™s the best action for a high-risk return request?"

    def vectorize_question(q):
        cursor.execute("""
            SELECT dbms_vector_chain.utl_to_embedding(
                :q,
                JSON('{"provider":"database","model":"DEMO_MODEL","dimensions":384}')
            ) FROM DUAL
        """, {'q': q})
        return cursor.fetchone()[0]

    print("Processing question using AI Vector Search...")

    try:
        q_vec = vectorize_question(question)

        # Retrieve the top recommendation chunks for THIS request (both sizes mixed),
        # ranked by vector similarity to the question.
        cursor.execute("""
            SELECT CHUNK_ID, CHUNK_TEXT
            FROM RETURN_CHUNKS
            WHERE REQUEST_ID = :rid
            AND CHUNK_VECTOR IS NOT NULL
            ORDER BY VECTOR_DISTANCE(CHUNK_VECTOR, :qv, COSINE)
            FETCH FIRST 5 ROWS ONLY
        """, {'rid': request_id, 'qv': q_vec})
        rec_hits = [
            (r[0], r[1].read() if isinstance(r[1], oracledb.LOB) else r[1])
            for r in cursor.fetchall()
        ]

        # If for some reason we didn't find chunks, fall back to the full recommendation text
        if not rec_hits:
            rec_hits = [(0, recommendations)]

        # Clean text for prompting
        cleaned = [re.sub(r'[^\w\s\d.,\-\'"]', ' ', t).strip() for _, t in rec_hits]
        docs_as_one_string = "\n=========\n".join(cleaned) + "\n=========\n"

        # Prepare compact RAG prompt (re-using the policy rules already loaded earlier)
        available_rules_text = "\n".join([
            f"{row['RULE_ID']}: {row['RULE_CODE']} | {row['RULE_DESCRIPTION']} | Applies To: {row['APPLIES_TO']}"
            for _, row in df_policy_rules.iterrows()
        ])
        customer_profile_text = "\n".join([
            f"- {k.replace('_',' ').title()}: {v}"
            for k, v in customer_json.items() if k not in ["returnRequests", "_metadata"]
        ])
        return_request_text = "\n".join([
            f"- {k.replace('_',' ').title()}: {v}"
            for k, v in ret_req.items() if k != "recommendation"
        ])

        rag_prompt = f"""<s>[INST] <<SYS>>You are a Retail Decision AI.
    Use only the provided context (below). No outside sources. Keep output under 300 words.
    Be specific and actionable.</SYS>> [/INST]
    [INST]
    Question: "{question}"

    Available Policy Rules:
    {available_rules_text}

    Customer Profile:
    {customer_profile_text}

    Return Request:
    {return_request_text}

    Context:
    {docs_as_one_string}

    Tasks:
    1) Provide a direct answer (APPROVE / REQUEST INFO / DENY + short rationale).
    2) Tie reasoning to the most relevant policy rule(s) or customer/request signals.
    3) Note any evidence gaps and what to collect (if applicable).
    [/INST]"""

        print("Generating AI response...")

        genai_client = oci.generative_ai_inference.GenerativeAiInferenceClient(
            config=oci.config.from_file(os.getenv("OCI_CONFIG_PATH","~/.oci/config")),
            service_endpoint=os.getenv("ENDPOINT")
        )
        chat_detail = oci.generative_ai_inference.models.ChatDetails(
            compartment_id=os.getenv("COMPARTMENT_OCID"),
            chat_request=oci.generative_ai_inference.models.GenericChatRequest(
                messages=[oci.generative_ai_inference.models.UserMessage(
                    content=[oci.generative_ai_inference.models.TextContent(text=rag_prompt)]
                )],
                temperature=0.0,
                top_p=0.9
            ),
            serving_mode=oci.generative_ai_inference.models.OnDemandServingMode(
                model_id="meta.llama-3.2-90b-vision-instruct"
            )
        )

        chat_response = genai_client.chat(chat_detail)
        rag_answer = chat_response.data.chat_response.choices[0].message.content[0].text
        rag_answer = re.sub(r'[^\w\s\d.,\-\'"]', ' ', rag_answer)

        print("\nðŸ¤– AI Retail RAG Response:")
        print(rag_answer)

        # Traceability: print which chunks were retrieved
        print("\nðŸ“‘ Retrieved Chunks Used in Response:")
        for cid, text in rec_hits:
            # decode our synthetic CHUNK_ID so you can see the source size & offset
            # CHUNK_ID = size * 1,000,000 + offset
            size = cid // 1000000
            offset = cid % 1000000
            preview = text[:120].replace("\n", " ") + ("..." if len(text) > 120 else "")
            print(f"[Chunk {cid} | size={size}w | offset={offset}] : {preview}")

    except Exception as e:
        print(f"RAG flow error: {e}")
    </copy>
    ```

2. Click the "Run" button to execute the code.

    ![ask question](./images/ask-question.png " ")

3. Review the result.

    >*Note:* Your result may be different due to non-deterministic character of generative AI.

    ![rag](./images/rag.png " ")

## Summary

Congratulations! You implemented a RAG process in Oracle Database 23ai using Python.

To summarize:

* You created a function to connect to Oracle Database 23ai using the Oracle Python driver `oracledb`.
* You created a function to retrieve customer data.
* You created a function to connect to OCI Generative AI and create a first recommendation.
* You created a function to create embeddings of the customer data using Oracle Database 23ai.
* And finally, you implemented a RAG process in Oracle Database 23ai using Python.

Congratulations, you completed the lab!

You may now proceed to the next lab.

## Learn More

* [Code with Python](https://www.oracle.com/developer/python-developers/)
* [Oracle Database 23ai Documentation](https://docs.oracle.com/en/database/oracle/oracle-database/23/)

## Acknowledgements
* **Authors** - Francis Regalado, Uma Kumar
* **Contributors** - Kevin Lazarz
* **Last Updated By/Date** - Richard Piantini Cid, September 2025